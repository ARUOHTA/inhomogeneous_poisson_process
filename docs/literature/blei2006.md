\title{
Variational Inference for Dirichlet Process Mixtures
}

\author{
David M. Blei* Michael I. Jordan \({ }^{\dagger}\)
}

\begin{abstract}
Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP mixtures has enabled the application of nonparametric Bayesian methods to a variety of practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it is important to explore alternatives. One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, variational methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001; Blei et al. 2003). In this paper, we present a variational inference algorithm for DP mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms for DP mixtures of Gaussians and present an application to a large-scale image analysis problem.
\end{abstract}

Keywords: Dirichlet processes, hierarchical models, variational inference, image processing, Bayesian computation

\section*{1 Introduction}

The methodology of Monte Carlo Markov chain (MCMC) sampling has energized Bayesian statistics for more than a decade, providing a systematic approach to the computation of likelihoods and posterior distributions, and permitting the deployment of Bayesian methods in a rapidly growing number of applied problems. However, while an unquestioned success story, MCMC is not an unqualified one-MCMC methods can be slow to converge and their convergence can be difficult to diagnose. While further research on sampling is needed, it is also important to explore alternatives, particularly in the context of large-scale problems.

One such class of alternatives is provided by variational inference methods (Ghahramani and Beal 2001; Jordan et al. 1999; Opper and Saad 2001; Wainwright and Jordan 2003; Wiegerinck 2000). Like MCMC, variational inference methods have their roots in statistical physics, and, in contradistinction to MCMC methods, they are deterministic. The basic idea of variational inference is to formulate the computation of a marginal or conditional probability in terms of an optimization problem. This (generally intractable) problem is then "relaxed," yielding a simplified optimization problem that

\footnotetext{
*School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, http://www.cs.berkeley.edu/ ~ blei/
\({ }^{\dagger}\) Department of Statistics and Computer Science Division, University of California, Berkeley, CA, http://www.cs.berkeley.edu/~jordan/
}
depends on a number of free parameters, known as variational parameters. Solving for the variational parameters gives an approximation to the marginal or conditional probabilities of interest.

Variational inference methods have been developed principally in the context of the exponential family, where the convexity properties of the natural parameter space and the cumulant function yield an elegant general variational formalism
(Wainwright and Jordan 2003). For example, variational methods have been developed for parametric hierarchical Bayesian models based on general exponential family specifications (Ghahramani and Beal 2001). MCMC methods have seen much wider application. In particular, the development of MCMC algorithms for nonparametric models such as the Dirichlet process has led to increased interest in nonparametric Bayesian methods. In the current paper, we aim to close this gap by developing variational methods for Dirichlet process mixtures.

The Dirichlet process (DP), introduced in Ferguson (1973), is a measure on measures. The DP is parameterized by a base distribution \(G_{0}\) and a positive scaling parameter \(\alpha .^{1}\) Suppose we draw a random measure \(G\) from a Dirichlet process, and independently draw \(N\) random variables \(\eta_{n}\) from \(G\) :
\[
\begin{aligned}
G \mid\left\{G_{0}, \alpha\right\} & \sim \operatorname{DP}\left(G_{0}, \alpha\right) \\
\eta_{n} & \sim G, \quad n \in\{1, \ldots, N\} .
\end{aligned}
\]

Marginalizing out the random measure \(G\), the joint distribution of \(\left\{\eta_{1}, \ldots, \eta_{N}\right\}\) follows a P贸lya urn scheme (Blackwell and MacQueen 1973). Positive probability is assigned to configurations in which different \(\eta_{n}\) take on identical values; moreover, the underlying random measure \(G\) is discrete with probability one. This is seen most directly in the stick-breaking representation of the DP , in which \(G\) is represented explicitly as an infinite sum of atomic measures (Sethuraman 1994).

The Dirichlet process mixture model (Antoniak 1974) adds a level to the hierarchy by treating \(\eta_{n}\) as the parameter of the distribution of the \(n\)th observation. Given the discreteness of \(G\), the DP mixture has an interpretation as a mixture model with an unbounded number of mixture components.

Given a sample \(\left\{x_{1}, \ldots, x_{N}\right\}\) from a DP mixture, our goal is to compute the predictive density:
\[
p\left(x \mid x_{1}, \ldots, x_{N}, \alpha, G_{0}\right)=\int p(x \mid \eta) p\left(\eta \mid x_{1}, \ldots, x_{N}, \alpha, G_{0}\right) d \eta
\]

As in many hierarchical Bayesian models, the posterior distribution \(p\left(\eta \mid x_{1}, \ldots, x_{N}, G_{0}, \alpha\right)\) is complicated and is not available in a closed form. MCMC provides one class of approximations for this posterior and the predictive density (MacEachern 1994; Escobar and West 1995; Neal 2000).

\footnotetext{
\({ }^{1}\) Ferguson (1973) parameterizes the Dirichlet process by a single base measure, which is \(\alpha G_{0}\) in our notation.
}

In this paper, we present a variational inference algorithm for DP mixtures based on the stick-breaking representation of the underlying DP. The algorithm involves two probability distributions - the posterior distribution \(p\) and a variational distribution \(q\). The latter is endowed with free variational parameters, and the algorithmic problem is to adjust these parameters so that \(q\) approximates \(p\). We also use a stick-breaking representation for \(q\), but in this case we truncate the representation to yield a finitedimensional representation. While in principle we could also truncate \(p\), turning the model into a finite-dimensional model, it is important to emphasize at the outset that this is not our approach-we truncate only the variational distribution.

The paper is organized as follows. In Section 2 we provide basic background on DP mixture models, focusing on the case of exponential family mixtures. In Section 3 we present a variational inference algorithms for DP mixtures. Section 4 overviews MCMC algorithms for the DP mixture, discussing algorithms based both on the P贸lya urn representation and the stick-breaking representation. Section 5 presents the results of experimental comparisons, Section 6 presents an analysis of natural image data, and Section 7 presents our conclusions.

\section*{2 Dirichlet process mixture models}

Let \(\eta\) be a continuous random variable, let \(G_{0}\) be a non-atomic probability distribution for \(\eta\), and let \(\alpha\) be a positive, real-valued scalar. A random measure \(G\) is distributed according to a Dirichlet process (DP) (Ferguson 1973), with scaling parameter \(\alpha\) and base distribution \(G_{0}\), if for all natural numbers \(k\) and \(k\)-partitions \(\left\{B_{1}, \ldots, B_{k}\right\}\),
\[
\left(G\left(B_{1}\right), G\left(B_{2}\right), \ldots, G\left(B_{k}\right)\right) \sim \operatorname{Dir}\left(\alpha G_{0}\left(B_{1}\right), \alpha G_{0}\left(B_{2}\right), \ldots, \alpha G_{0}\left(B_{k}\right)\right) .
\]

Integrating out \(G\), the joint distribution of the collection of variables \(\left\{\eta_{1}, \ldots, \eta_{n}\right\}\) exhibits a clustering effect; conditioning on \(n-1\) draws, the \(n\)th value is, with positive probability, exactly equal to one of those draws:
\[
p\left(\cdot \mid \eta_{1}, \ldots, \eta_{n-1}\right) \propto \alpha G_{0}(\cdot)+\sum_{i=1}^{n-1} \delta_{\eta_{i}}(\cdot) .
\]

Thus, the variables \(\left\{\eta_{1}, \ldots, \eta_{n-1}\right\}\) are randomly partitioned according to which variables are equal to the same value, with the distribution of the partition obtained from a P贸lya urn scheme (Blackwell and MacQueen 1973). Let \(\left\{\eta_{1}^{*}, \ldots, \eta_{|\mathbf{c}|}^{*}\right\}\) denote the distinct values of \(\left\{\eta_{1}, \ldots, \eta_{n-1}\right\}\), let \(\mathbf{c}=\left\{c_{1}, \ldots, c_{n-1}\right\}\) be assignment variables such that \(\eta_{i}=\eta_{c_{i}}^{*}\), and let \(|\mathbf{c}|\) denote the number of cells in the partition. The distribution of \(\eta_{n}\) follows the urn distribution:
\[
\eta_{n}=\left\{\begin{array}{rll}
\eta_{i}^{*} & \text { with prob. } & \frac{\left|\left\{j: c_{j}=i\right\}\right|}{n-1+\alpha} \\
\eta, \eta \sim G_{0} & \text { with prob. } & \frac{\alpha}{n-1+\alpha},
\end{array}\right.
\]
where \(\left|\left\{j: c_{j}=i\right\}\right|\) is the number of times the value \(\eta_{i}^{*}\) occurs in \(\left\{\eta_{1}, \ldots, \eta_{n-1}\right\}\).

In the Dirichlet process mixture model, the DP is used as a nonparametric prior in a hierarchical Bayesian specification (Antoniak 1974):
\[
\begin{aligned}
G \mid\left\{\alpha, G_{0}\right\} & \sim \mathrm{DP}\left(\alpha, G_{0}\right) \\
\eta_{n} \mid G & \sim G \\
X_{n} \mid \eta_{n} & \sim p\left(x_{n} \mid \eta_{n}\right) .
\end{aligned}
\]

Data generated from this model can be partitioned according to the distinct values of the parameter. Taking this view, the DP mixture has a natural interpretation as a flexible mixture model in which the number of components (i.e., the number of cells in the partition) is random and grows as new data are observed.

The definition of the DP via its finite dimensional distributions in Equation (2) reposes on the Kolmogorov consistency theorem (Ferguson 1973). Sethuraman (1994) provides a more explicit characterization of the DP in terms of a stick-breaking construction. Consider two infinite collections of independent random variables, \(V_{i} \sim \operatorname{Beta}(1, \alpha)\) and \(\eta_{i}^{*} \sim G_{0}\), for \(i=\{1,2, \ldots\}\). The stick-breaking representation of \(G\) is as follows:
\[
\begin{aligned}
& \pi_{i}(\mathbf{v})=v_{i} \prod_{j=1}^{i-1}\left(1-v_{j}\right) \\
& G=\sum_{i=1}^{\infty} \pi_{i}(\mathbf{v}) \delta_{\eta_{i}^{*}}
\end{aligned}
\]

This representation of the DP makes clear that \(G\) is discrete (with probability one); the support of \(G\) consists of a countably infinite set of atoms, drawn independently from \(G_{0}\). The mixing proportions \(\pi_{i}(\mathbf{v})\) are given by successively breaking a unit length "stick" into an infinite number of pieces. The size of each successive piece, proportional to the rest of the stick, is given by an independent draw from a \(\operatorname{Beta}(1, \alpha)\) distribution.

In the DP mixture, the vector \(\pi(\mathbf{v})\) comprises the infinite vector of mixing proportions and \(\left\{\eta_{1}^{*}, \eta_{2}^{*}, \ldots\right\}\) are the atoms representing the mixture components. Let \(Z_{n}\) be an assignment variable of the mixture component with which the data point \(x_{n}\) is associated. The data can be described as arising from the following process:
1. Draw \(V_{i} \mid \alpha \sim \operatorname{Beta}(1, \alpha), \quad i=\{1,2, \ldots\}\)
2. Draw \(\eta_{i}^{*} \mid G_{0} \sim G_{0}, \quad i=\{1,2, \ldots\}\)
3. For the \(n\)th data point:
(a) Draw \(Z_{n} \mid\left\{v_{1}, v_{2}, \ldots\right\} \sim \operatorname{Mult}(\pi(\mathbf{v}))\).
(b) Draw \(X_{n} \mid z_{n} \sim p\left(x_{n} \mid \eta_{z_{n}}^{*}\right)\).

In this paper, we restrict ourselves to DP mixtures for which the observable data are drawn from an exponential family distribution, and where the base distribution for the DP is the corresponding conjugate prior.

\begin{figure}
\includegraphics[max width=\textwidth]{https://cdn.mathpix.com/cropped/b4ca9d49-b8eb-48a5-8d65-5ba75d4210f0-05.jpg?height=410&width=738&top_left_y=474&top_left_x=693}
\captionsetup{labelformat=empty}
\caption{Figure 1: Graphical model representation of an exponential family DP mixture. Nodes denote random variables, edges denote possible dependence, and plates denote replication.}
\end{figure}

The stick-breaking construction for the DP mixture is depicted as a graphical model in Figure 1. The conditional distributions of \(V_{k}\) and \(Z_{n}\) are as described above. The distribution of \(X_{n}\) conditional on \(Z_{n}\) and \(\left\{\eta_{1}^{*}, \eta_{2}^{*}, \ldots\right\}\) is
\[
p\left(x_{n} \mid z_{n}, \eta_{1}^{*}, \eta_{2}^{*}, \ldots\right)=\prod_{i=1}^{\infty}\left(h\left(x_{n}\right) \exp \left\{\eta_{i}^{* T} x_{n}-a\left(\eta_{i}^{*}\right)\right\}\right)^{\mathbf{1}\left[z_{n}=i\right]}
\]
where \(a\left(\eta_{i}^{*}\right)\) is the appropriate cumulant function and we assume for simplicity that \(x\) is the sufficient statistic for the natural parameter \(\eta\).

The vector of sufficient statistics of the corresponding conjugate family is \(\left(\eta^{* T},-a\left(\eta^{*}\right)\right)^{T}\). The base distribution is
\[
p\left(\eta^{*} \mid \lambda\right)=h\left(\eta^{*}\right) \exp \left\{\lambda_{1}^{T} \eta^{*}+\lambda_{2}\left(-a\left(\eta^{*}\right)\right)-a(\lambda)\right\}
\]
where we decompose the hyperparameter \(\lambda\) such that \(\lambda_{1}\) contains the first \(\operatorname{dim}\left(\eta^{*}\right)\) components and \(\lambda_{2}\) is a scalar.

\section*{3 Variational inference for DP mixtures}

There is no direct way to compute the posterior distribution under a DP mixture prior. Approximate inference methods are required for DP mixtures and Markov chain Monte Carlo (MCMC) sampling methods have become the methodology of choice (MacEachern 1994; Escobar and West 1995; MacEachern 1998; Neal 2000; Ishwaran and James 2001).

Variational inference provides an alternative, deterministic methodology for approximating likelihoods and posteriors (Wainwright and Jordan 2003). Consider a model with hyperparameters \(\theta\), latent variables \(\mathbf{W}=\left\{W_{1}, \ldots, W_{M}\right\}\), and observations \(\mathbf{x}= \left\{x_{1}, \ldots, x_{N}\right\}\). The posterior distribution of the latent variables is:
\[
p(\mathbf{w} \mid \mathbf{x}, \theta)=\exp \{\log p(\mathbf{x}, \mathbf{w} \mid \theta)-\log p(\mathbf{x} \mid \theta)\}
\]

Working directly with this posterior is typically precluded by the need to compute the normalizing constant. The log marginal probability of the observations is:
\[
\log p(\mathbf{x} \mid \theta)=\log \int p(\mathbf{w}, \mathbf{x} \mid \theta) d \mathbf{w}
\]
which may be difficult to compute given that the latent variables become dependent when conditioning on observed data.

MCMC algorithms circumvent this computation by constructing an approximate posterior based on samples from a Markov chain whose stationary distribution is the posterior of interest. Gibbs sampling is the simplest MCMC algorithm; one iteratively samples each latent variable conditioned on the previously sampled values of the other latent variables:
\[
p\left(w_{i} \mid \mathbf{w}_{-i}, \mathbf{x}, \theta\right)=\exp \left\{\log p(\mathbf{w}, \mathbf{x} \mid \theta)-\log p\left(\mathbf{w}_{-i}, \mathbf{x} \mid \theta\right)\right\}
\]

The normalizing constants for these conditional distributions are assumed to be available analytically for settings in which Gibbs sampling is appropriate.

Variational inference is based on reformulating the problem of computing the posterior distribution as an optimization problem, perturbing (or, "relaxing") that problem, and finding solutions to the perturbed problem (Wainwright and Jordan 2003). In this paper, we work with a particular class of variational methods known as mean-field methods. These are based on optimizing Kullback-Leibler (KL) divergence with respect to a so-called variational distribution. In particular, let \(q_{\nu}(\mathbf{w})\) be a family of distributions indexed by a variational parameter \(\nu\). We aim to minimize the KL divergence between \(q_{\nu}(\mathbf{w})\) and \(p(\mathbf{w} \mid \mathbf{x}, \theta)\) :
\[
\mathrm{D}\left(q_{\nu}(\mathbf{w}) \| p(\mathbf{w} \mid \mathbf{x}, \theta)\right)=\mathrm{E}_{q}\left[\log q_{\nu}(\mathbf{W})\right]-\mathrm{E}_{q}[\log p(\mathbf{W}, \mathbf{x} \mid \theta)]+\log p(\mathbf{x} \mid \theta)
\]
where here and elsewhere in the paper we omit the variational parameters \(\nu\) when using \(q\) as a subscript of an expectation. Notice that the problematic marginal probability does not depend on the variational parameters; it can be ignored in the optimization.

The minimization in Equation (11) can be cast alternatively as the maximization of a lower bound on the log marginal likelihood:
\[
\log p(\mathbf{x} \mid \theta) \geq \mathrm{E}_{q}[\log p(\mathbf{W}, \mathbf{x} \mid \theta)]-\mathrm{E}_{q}\left[\log q_{\nu}(\mathbf{W})\right]
\]

The gap in this bound is the divergence between \(q_{\nu}(\mathbf{w})\) and the true posterior.
For the mean-field framework to yield a computationally effective inference method, it is necessary to choose a family of distributions \(q_{\nu}(\mathbf{w})\) such that we can tractably optimize Equation (11). In constructing that family, one typically breaks some of the dependencies between latent variables that make the true posterior difficult to compute. In the next sections, we consider fully-factorized variational distributions which break all of the dependencies.

\subsection*{3.1 Mean field variational inference in exponential families}

For each latent variable, let us assume that the conditional distribution \(p\left(w_{i} \mid \mathbf{w}_{-i}, \mathbf{x}, \theta\right)\) is a member of the exponential family \({ }^{2}\) :
\[
p\left(w_{i} \mid \mathbf{w}_{-i}, \mathbf{x}, \theta\right)=h\left(w_{i}\right) \exp \left\{g_{i}\left(\mathbf{w}_{-i}, \mathbf{x}, \theta\right)^{T} w_{i}-a\left(g_{i}\left(\mathbf{w}_{-i}, \mathbf{x}, \theta\right)\right)\right\},
\]
where \(g_{i}\left(\mathbf{w}_{-i}, \mathbf{x}, \theta\right)\) is the natural parameter for \(w_{i}\) when conditioning on the remaining latent variables and the observations.

In this setting it is natural to consider the following family of distributions as meanfield variational approximations (Ghahramani and Beal 2001):
\[
q_{\boldsymbol{\nu}}(\mathbf{w})=\prod_{i=1}^{M} \exp \left\{\nu_{i}^{T} w_{i}-a\left(w_{i}\right)\right\}
\]
where \(\boldsymbol{\nu}=\left\{\nu_{1}, \nu_{2}, \ldots, \nu_{M}\right\}\) are variational parameters. Indeed, it turns out that the variational algorithm that we obtain using this fully-factorized family is reminiscent of Gibbs sampling. In particular, as we show in Appendix 7, the optimization of KL divergence with respect to a single variational parameter \(\nu_{i}\) is achieved by computing the following expectation:
\[
\nu_{i}=\mathrm{E}_{q}\left[g_{i}\left(\mathbf{W}_{-i}, \mathbf{x}, \theta\right)\right] .
\]

Repeatedly updating each parameter in turn by computing this expectation amounts to performing coordinate ascent in the KL divergence.

Notice the interesting relationship of this algorithm to the Gibbs sampler. In Gibbs sampling, we iteratively draw the latent variables \(w_{i}\) from the distribution \(p\left(w_{i} \mid \mathbf{w}_{-i}, \mathbf{x}, \theta\right)\). In mean-field variational inference, we iteratively update the variational parameter \(\nu_{i}\) by setting it equal to the expected value of \(g_{i}\left(\mathbf{w}_{-i}, \mathbf{x}, \theta\right)\). This expectation is computed under the variational distribution.

\subsection*{3.2 DP mixtures}

In this section we develop a mean-field variational algorithm for the DP mixture. Our algorithm is based on the stick-breaking representation of the DP mixture (see Figure 1). In this representation the latent variables are the stick lengths, the atoms, and the cluster assignments: \(\mathbf{W}=\left\{\mathbf{V}, \boldsymbol{\eta}^{*}, \mathbf{Z}\right\}\). The hyperparameters are the scaling parameter and the parameter of the conjugate base distribution: \(\theta=\{\alpha, \lambda\}\).

Following the general recipe in Equation (12), we write the variational bound on the

\footnotetext{
\({ }^{2}\) Examples of models in which \(p\left(w_{i} \mid \mathbf{w}_{-i}, \mathbf{x}, \theta\right)\) is an exponential family distribution include hidden Markov models, mixture models, state space models, and hierarchical Bayesian models with conjugate and mixture of conjugate priors.
}
log marginal probability of the data:
\[
\begin{aligned}
\log p(\mathbf{x} \mid \alpha, \lambda) \geq & \mathrm{E}_{q}[\log p(\mathbf{V} \mid \alpha)]+\mathrm{E}_{q}\left[\log p\left(\boldsymbol{\eta}^{*} \mid \lambda\right)\right] \\
& +\sum_{n=1}^{N}\left(\mathrm{E}_{q}\left[\log p\left(Z_{n} \mid \mathbf{V}\right)\right]+\mathrm{E}_{q}\left[\log p\left(x_{n} \mid Z_{n}\right)\right]\right) \\
& -\mathrm{E}_{q}\left[\log q\left(\mathbf{V}, \boldsymbol{\eta}^{*}, \mathbf{Z}\right)\right]
\end{aligned}
\]

To exploit this bound, we must find a family of variational distributions that approximates the distribution of the infinite-dimensional random measure \(G\), where the random measure is expressed in terms of the infinite sets \(\mathbf{V}=\left\{V_{1}, V_{2}, \ldots\right\}\) and \(\boldsymbol{\eta}^{*}=\left\{\eta_{1}^{*}, \eta_{2}^{*}, \ldots\right\}\). We do this by considering truncated stick-breaking representations. Thus, we fix a value \(T\) and let \(q\left(v_{T}=1\right)=1\); this implies that the mixture proportions \(\pi_{t}(\mathbf{v})\) are equal to zero for \(t>T\) (see Equation 5).

Truncated stick-breaking representations have been considered previously by Ishwaran and James (2001) in the context of sampling-based inference for an approximation to the DP mixture model. Note that our use of truncation is rather different. In our case, the model is a full Dirichlet process and is not truncated; only the variational distribution is truncated. The truncation level \(T\) is a variational parameter which can be freely set; it is not a part of the prior model specification (see Section 5).

We thus propose the following factorized family of variational distributions for meanfield variational inference:
\[
q\left(\mathbf{v}, \boldsymbol{\eta}^{*}, \mathbf{z}\right)=\prod_{t=1}^{T-1} q_{\gamma_{t}}\left(v_{t}\right) \prod_{t=1}^{T} q_{\tau_{t}}\left(\eta_{t}^{*}\right) \prod_{n=1}^{N} q_{\phi_{n}}\left(z_{n}\right)
\]
where \(q_{\gamma_{t}}\left(v_{t}\right)\) are beta distributions, \(q_{\tau_{t}}\left(\eta_{t}^{*}\right)\) are exponential family distributions with natural parameters \(\tau_{t}\), and \(q_{\phi_{n}}\left(z_{n}\right)\) are multinomial distributions. In the notation of Section 3.1, the free variational parameters are
\[
\boldsymbol{\nu}=\left\{\gamma_{1}, \ldots, \gamma_{T-1}, \tau_{1}, \ldots, \tau_{T}, \phi_{1}, \ldots, \phi_{N}\right\} .
\]

It is important to note that there is a different variational parameter for each latent variable under the variational distribution. For example, the choice of the mixture component \(z_{n}\) for the \(n\)th data point is governed by a multinomial distribution indexed by a variational parameter \(\phi_{n}\). This reflects the conditional nature of variational inference.

\section*{Coordinate ascent algorithm}

In this section we present an explicit coordinate ascent algorithm for optimizing the bound in Equation (16) with respect to the variational parameters.

All of the terms in the bound involve standard computations in the exponential family, except for the third term. We rewrite the third term using indicator random
variables:
\[
\begin{aligned}
\mathrm{E}_{q}\left[\log p\left(Z_{n} \mid \mathbf{V}\right)\right] & =\mathrm{E}_{q}\left[\log \left(\prod_{i=1}^{\infty}\left(1-V_{i}\right)^{\mathbf{1}\left[Z_{n}>i\right]} V_{i}^{\mathbf{1}\left[Z_{n}=i\right]}\right)\right] \\
& =\sum_{i=1}^{\infty} q\left(z_{n}>i\right) \mathrm{E}_{q}\left[\log \left(1-V_{i}\right)\right]+q\left(z_{n}=i\right) \mathrm{E}_{q}\left[\log V_{i}\right]
\end{aligned}
\]

Recall that \(\mathrm{E}_{q}\left[\log \left(1-V_{T}\right)\right]=0\) and \(q\left(z_{n}>T\right)=0\). Consequently, we can truncate this summation at \(t=T\) :
\[
\mathrm{E}_{q}\left[\log p\left(Z_{n} \mid \mathbf{V}\right)\right]=\sum_{i=1}^{T} q\left(z_{n}>i\right) \mathrm{E}_{q}\left[\log \left(1-V_{i}\right)\right]+q\left(z_{n}=i\right) \mathrm{E}_{q}\left[\log V_{i}\right]
\]
where
\[
\begin{aligned}
q\left(z_{n}=i\right) & =\phi_{n, i} \\
q\left(z_{n}>i\right) & =\sum_{j=i+1}^{T} \phi_{n, j} \\
\mathrm{E}_{q}\left[\log V_{i}\right] & =\Psi\left(\gamma_{i, 1}\right)-\Psi\left(\gamma_{i, 1}+\gamma_{i, 2}\right) \\
\mathrm{E}_{q}\left[\log \left(1-V_{i}\right)\right] & =\Psi\left(\gamma_{i, 2}\right)-\Psi\left(\gamma_{i, 1}+\gamma_{i, 2}\right)
\end{aligned}
\]

The digamma function, denoted by \(\Psi\), arises from the derivative of the \(\log\) normalization factor in the beta distribution.

We now use the general expression in Equation (15) to derive a mean-field coordinate ascent algorithm. This yields:
\[
\begin{aligned}
\gamma_{t, 1} & =1+\sum_{n} \phi_{n, t} \\
\gamma_{t, 2} & =\alpha+\sum_{n} \sum_{j=t+1}^{T} \phi_{n, j} \\
\tau_{t, 1} & =\lambda_{1}+\sum_{n} \phi_{n, t} x_{n} \\
\tau_{t, 2} & =\lambda_{2}+\sum_{n} \phi_{n, t} \\
\phi_{n, t} & \propto \exp \left(S_{t}\right)
\end{aligned}
\]
for \(t \in\{1, \ldots, T\}\) and \(n \in\{1, \ldots, N\}\), where
\[
S_{t}=\mathrm{E}_{q}\left[\log V_{t}\right]+\sum_{i=1}^{t-1} \mathrm{E}_{q}\left[\log \left(1-V_{i}\right)\right]+\mathrm{E}_{q}\left[\eta_{t}^{*}\right]^{T} X_{n}-\mathrm{E}_{q}\left[a\left(\eta_{t}^{*}\right)\right]
\]

Iterating these updates optimizes Equation (16) with respect to the variational parameters defined in Equation (17).

Practical applications of variational methods must address initialization of the variational distribution. While the algorithm yields a bound for any starting values of the variational parameters, poor choices of initialization can lead to local maxima that yield poor bounds. We initialize the variational distribution by incrementally updating the parameters according to a random permutation of the data points. (This can be viewed as a variational version of sequential importance sampling). We run the algorithm multiple times and choose the final parameter settings that give the best bound on the marginal likelihood.

To compute the predictive distribution, we use the variational posterior in a manner analogous to the way that the empirical approximation is used by an MCMC sampling algorithm. The predictive distribution is:
\[
p\left(x_{N+1} \mid \mathbf{x}, \alpha, \lambda\right)=\int\left(\sum_{t=1}^{\infty} \pi_{t}(\mathbf{v}) p\left(x_{N+1} \mid \eta_{t}^{*}\right)\right) d P\left(\mathbf{v}, \boldsymbol{\eta}^{*} \mid \mathbf{x}, \lambda, \alpha\right)
\]

Under the factorized variational approximation to the posterior, the distribution of the atoms and the stick lengths are decoupled and the infinite sum is truncated. Consequently, we can approximate the predictive distribution with a product of expectations which are straightforward to compute under the variational approximation,
\[
p\left(x_{N+1} \mid \mathbf{x}, \alpha, \lambda\right) \approx \sum_{t=1}^{T} \mathrm{E}_{q}\left[\pi_{t}(\mathbf{V})\right] \mathrm{E}_{q}\left[p\left(x_{N+1} \mid \eta_{t}^{*}\right)\right]
\]
where \(q\) depends implicitly on \(\mathbf{x}, \alpha\), and \(\lambda\).
Finally, we remark on two possible extensions. First, when \(G_{0}\) is not conjugate, a simple coordinate ascent update for \(\tau_{i}\) may not be available, particularly when \(p\left(\eta_{i}^{*} \mid \mathbf{z}, \mathbf{x}, \lambda\right)\) is not in the exponential family. However, such an update is available for the special case of \(G_{0}\) being a mixture of conjugate distributions. Second, it is often important in applications to integrate over a diffuse prior on the scaling parameter \(\alpha\). As we show in Appendix 7, it is straightforward to extend the variational algorithm to include a gamma prior on \(\alpha\).

\section*{4 Gibbs sampling}

For comparison to variational inference, we review the collapsed Gibbs sampler and blocked Gibbs sampler for DP mixtures.

\subsection*{4.1 Collapsed Gibbs sampling}

The collapsed Gibbs sampler for a DP mixture with conjugate base distribution (MacEachern 1994) integrates out the random measure \(G\) and distinct parameter values \(\left\{\eta_{1}^{*}, \ldots, \eta_{|\mathbf{c}|}^{*}\right\}\). The Markov chain is thus defined only on the latent partition \(\mathbf{c}=\left\{c_{1}, \ldots, c_{N}\right\}\). (Recall that \(|\mathbf{c}|\) denotes the number of cells in the partition.)

The algorithm iteratively samples each assignment variable \(C_{n}\), for \(n \in\{1, \ldots, N\}\), conditional on the other cells in the partition, \(\mathbf{c}_{-n}\). The assignment \(C_{n}\) can be one of \(\left|\mathbf{c}_{-n}\right|+1\) values: either the \(n\)th data point is in a cell with other data points, or in a cell by itself.

Exchangeability implies that \(C_{n}\) has the following multinomial distribution:
\[
p\left(c_{n}=k \mid \mathbf{x}, \mathbf{c}_{-n}, \lambda, \alpha\right) \propto p\left(x_{n} \mid \mathbf{x}_{-n}, \mathbf{c}_{-n}, c_{n}=k, \lambda\right) p\left(c_{n}=k \mid \mathbf{c}_{-n}, \alpha\right)
\]

The first term is a ratio of normalizing constants of the posterior distribution of the \(k\) th parameter, one including and one excluding the \(n\)th data point:
\[
\begin{aligned}
& p\left(x_{n} \mid \mathbf{x}_{-n}, \mathbf{c}_{-n}, c_{n}=k, \lambda\right)= \\
& \quad \frac{\exp \left\{a\left(\lambda_{1}+\sum_{m \neq n} \mathbf{1}\left[c_{m}=k\right] x_{m}+x_{n}, \lambda_{2}+\sum_{m \neq n} \mathbf{1}\left[c_{m}=k\right]+1\right)\right\}}{\exp \left\{a\left(\lambda_{1}+\sum_{m \neq n} \mathbf{1}\left[c_{m}=k\right] x_{m}, \lambda_{2}+\sum_{m \neq n} \mathbf{1}\left[c_{m}=k\right]\right)\right\}}
\end{aligned}
\]

The second term is given by the P贸lya urn scheme:
\[
p\left(c_{n}=k \mid \mathbf{c}_{-n}\right) \propto \begin{cases}\left|\left\{j: c_{-n, j}=k\right\}\right| & \text { if } k \text { is an existing cell in the partition } \\ \alpha & \text { if } k \text { is a new cell in the partition }\end{cases}
\]
where \(\left|\left\{j: c_{-n, j}=k\right\}\right|\) denotes the number of data points in the \(k\) th cell of the partition \(\mathbf{c}_{-n}\).

Once this chain has reached its stationary distribution, we collect \(B\) samples \(\left\{\mathbf{c}_{1}, \ldots, \mathbf{c}_{B}\right\}\) to approximate the posterior. The approximate predictive distribution is an average of the predictive distributions across the Monte Carlo samples:
\[
p\left(x_{N+1} \mid x_{1}, \ldots, x_{N}, \alpha, \lambda\right)=\frac{1}{B} \sum_{b=1}^{B} p\left(x_{N+1} \mid \mathbf{c}_{b}, \mathbf{x}, \alpha, \lambda\right)
\]

For a given sample, that distribution is
\[
p\left(x_{N+1} \mid \mathbf{c}_{b}, \mathbf{x}, \alpha, \lambda\right)=\sum_{k=1}^{\left|\mathbf{c}_{b}\right|+1} p\left(c_{N+1}=k \mid \mathbf{c}_{b}, \alpha\right) p\left(x_{N+1} \mid \mathbf{c}_{b}, \mathbf{x}, c_{N+1}=k, \lambda\right) .
\]

When \(G_{0}\) is not conjugate, the distribution in Equation (25) does not have a simple closed form. Effective algorithms for handling this case are given in Neal (2000).

\subsection*{4.2 Blocked Gibbs sampling}

In the collapsed Gibbs sampler, the assignment variable \(C_{n}\) is drawn from a distribution that depends on the most recently sampled values of the other assignment variables. Consequently, these variables must be updated one at a time which can potentially slow down the algorithm when compared to a blocking strategy. To this end, Ishwaran and James (2001) developed a blocked Gibbs sampling algorithm based on the stick-breaking representation of Figure 1.

The main issue to face in developing a blocked Gibbs sampler for the stick-breaking DP mixture is that one needs to sample the infinite collection of stick lengths \(\mathbf{V}\) before sampling the finite collection of cluster assignments \(\mathbf{Z}\). Ishwaran and James (2001) face this issue by defining a truncated Dirichlet process (TDP) in which \(V_{K-1}\) is set equal to one for some fixed value \(K\). This yields \(\pi_{i}(\mathbf{V})=0\) for \(i \geq K\), and converts the infinite sum in Equation (5) into a finite sum. Ishwaran and James (2001) justify substituting a

TDP mixture model for a full DP mixture model by showing that the truncated process closely approximates a true Dirichlet process when the truncation level is chosen large relative to the number of data points.

In the TDP mixture, the state of the Markov chain consists of the beta variables \(\mathbf{V}=\left\{V_{1}, \ldots, V_{K-1}\right\}\), the mixture component parameters \(\boldsymbol{\eta}^{*}=\left\{\eta_{1}^{*}, \ldots, \eta_{K}^{*}\right\}\), and the indicator variables \(\mathbf{Z}=\left\{Z_{1}, \ldots, Z_{N}\right\}\). The blocked Gibbs sampler iterates between the following three steps:
1. For \(n \in\{1, \ldots, N\}\), independently sample \(Z_{n}\) from
\[
p\left(z_{n}=k \mid \mathbf{v}, \boldsymbol{\eta}^{*}, \mathbf{x}\right)=\pi_{k}(\mathbf{v}) p\left(x_{n} \mid \eta_{k}^{*}\right)
\]
2. For \(k \in\{1, \ldots, K\}\), independently sample \(V_{k}\) from \(\operatorname{Beta}\left(\gamma_{k, 1}, \gamma_{k, 2}\right)\), where
\[
\begin{aligned}
\gamma_{k, 1} & =1+\sum_{n=1}^{N} \mathbf{1}\left[z_{n}=k\right] \\
\gamma_{k, 2} & =\alpha+\sum_{i=k+1}^{K} \sum_{n=1}^{N} \mathbf{1}\left[z_{n}=i\right] .
\end{aligned}
\]

This step follows from the conjugacy between the multinomial distribution and the truncated stick-breaking construction, which is a generalized Dirichlet distribution (Connor and Mosimann 1969).
3. For \(k \in\{1, \ldots, K\}\), independently sample \(\eta_{k}^{*}\) from \(p\left(\eta_{k}^{*} \mid \tau_{k}\right)\). This distribution is in the same family as the base distribution, with parameters
\[
\begin{aligned}
\tau_{k, 1} & =\lambda_{1}+\sum_{i \neq n} \mathbf{1}\left[z_{i}=k\right] x_{i} \\
\tau_{k, 2} & =\lambda_{2}+\sum_{i \neq n} \mathbf{1}\left[z_{i}=k\right]
\end{aligned}
\]

After the chain has reached its stationary distribution, we collect \(B\) samples and construct an approximate predictive distribution. Again, this distribution is an average of the predictive distributions for each of the collected samples. The predictive distribution for a particular sample is
\[
p\left(x_{N+1} \mid \mathbf{z}, \mathbf{x}, \alpha, \lambda\right)=\sum_{k=1}^{K} \mathrm{E}\left[\pi_{i}(\mathbf{V}) \mid \gamma_{1}, \ldots, \gamma_{k}\right] p\left(x_{N+1} \mid \tau_{k}\right)
\]
where \(\mathrm{E}\left[\pi_{i} \mid \gamma_{1}, \ldots, \gamma_{k}\right]\) is the expectation of the product of independent beta variables given in Equation (5). This distribution only depends on \(\mathbf{z}\); the other variables are needed in the Gibbs sampling procedure, but can be integrated out here. Note that this approximation has a form similar to the approximate predictive distribution under the variational distribution in Equation (23). In the variational case, however, the averaging is done parametrically via the variational distribution rather than by a Monte Carlo integral.

The TDP sampler readily handles non-conjugacy of \(G_{0}\), provided that there is a method of sampling \(\eta_{i}^{*}\) from its posterior.

\begin{figure}
\includegraphics[max width=\textwidth]{https://cdn.mathpix.com/cropped/b4ca9d49-b8eb-48a5-8d65-5ba75d4210f0-13.jpg?height=452&width=1263&top_left_y=498&top_left_x=433}
\captionsetup{labelformat=empty}
\caption{Figure 2: The approximate predictive distribution given by variational inference at different stages of the algorithm. The data are 100 points generated by a Gaussian DP mixture model with fixed diagonal covariance.}
\end{figure}

\section*{5 Empirical comparison}

Qualitatively, variational methods offer several potential advantages over Gibbs sampling. They are deterministic, and have an optimization criterion given by Equation (16) that can be used to assess convergence. In contrast, assessing convergence of a Gibbs sampler-namely, determining when the Markov chain has reached its stationary distribution - is an active field of research. Theoretical bounds on the mixing time are of little practical use, and there is no consensus on how to choose among the several empirical methods developed for this purpose (Robert and Casella 2004).

But there are several potential disadvantages of variational methods as well. First, the optimization procedure can fall prey to local maxima in the variational parameter space. Local maxima can be mitigated with restarts, or removed via the incorporation of additional variational parameters, but these strategies may slow the overall convergence of the procedure. Second, any given fixed variational representation yields only an approximation to the posterior. There are methods for considering hierarchies of variational representations that approach the posterior in the limit, but these methods may again incur serious computational costs. Lacking a theory by which these issues can be evaluated in the general setting of DP mixtures, we turn to experimental evaluation.

We studied the performance of the variational algorithm of Section 3 and the Gibbs samplers of Section 4 in the setting of DP mixtures of Gaussians with fixed inverse covariance matrix \(\Lambda\) (i.e., the DP mixes over the mean of the Gaussian). The natural conjugate base distribution for the DP is Gaussian, with covariance given by \(\Lambda / \lambda_{2}\) (see Equation 7).

Figure 2 provides an illustrative example of variational inference on a small problem involving 100 data points sampled from a two-dimensional DP mixture of Gaussians with diagonal covariance. Each panel in the figure plots the data and presents the

\begin{figure}
\includegraphics[max width=\textwidth]{https://cdn.mathpix.com/cropped/b4ca9d49-b8eb-48a5-8d65-5ba75d4210f0-14.jpg?height=830&width=740&top_left_y=472&top_left_x=659}
\captionsetup{labelformat=empty}
\caption{Figure 3: Mean convergence time and standard error across ten data sets per dimension for variational inference, TDP Gibbs sampling, and the collapsed Gibbs sampler.}
\end{figure}
predictive distribution given by the variational inference algorithm at a given iteration (see Equation (23)). The truncation level was set to 20. As seen in the first panel, the initialization of the variational parameters yields a largely flat distribution. After one iteration, the algorithm has found the modes of the predictive distribution and, after convergence, it has further refined those modes. Even though 20 mixture components are represented in the variational distribution, the fitted approximate posterior only uses five of them.

To compare the variational inference algorithm to the Gibbs sampling algorithms, we conducted a systematic set of simulation experiments in which the dimensionality of the data was varied from 5 to 50 . The covariance matrix was given by the autocorrelation matrix for a first-order autoregressive process, chosen so that the components are highly dependent ( \(\rho=0.9\) ). The base distribution was a zero-mean Gaussian with covariance appropriately scaled for comparison across dimensions. The scaling parameter \(\alpha\) was set equal to one.

In each case, we generated 100 data points from a DP mixture of Gaussians model of the chosen dimensionality and generated 100 additional points as held-out data. In testing on the held-out data, we treated each point as the 101st data point in the collection and computed its conditional probability using each algorithm's approximate predictive distribution.

\begin{table}
\begin{tabular}{l|lll} 
Dim & \multicolumn{3}{|c}{ Mean held out log probability (Std err) } \\
& Variational & Collapsed Gibbs & Truncated Gibbs \\
\hline 5 & \(-147.96(4.12)\) & \(-148.08(3.93)\) & \(-147.93(3.88)\) \\
10 & \(-266.59(7.69)\) & \(-266.29(7.64)\) & \(-265.89(7.66)\) \\
20 & \(-494.12(7.31)\) & \(-492.32(7.54)\) & \(-491.96(7.59)\) \\
30 & \(-721.55(8.18)\) & \(-720.05(7.92)\) & \(-720.02(7.96)\) \\
40 & \(-943.39(10.65)\) & \(-941.04(10.15)\) & \(-940.71(10.23)\) \\
50 & \(-1151.01(15.23)\) & \(-1148.51(14.78)\) & \(-1147.48(14.55)\)
\end{tabular}
\captionsetup{labelformat=empty}
\caption{Table 1: Average held-out log probability for the predictive distributions given by variational inference, TDP Gibbs sampling, and the collapsed Gibbs sampler.}
\end{table}

\begin{figure}
\includegraphics[max width=\textwidth]{https://cdn.mathpix.com/cropped/b4ca9d49-b8eb-48a5-8d65-5ba75d4210f0-15.jpg?height=609&width=1258&top_left_y=1370&top_left_x=423}
\captionsetup{labelformat=empty}
\caption{Figure 4: The optimal bound on the \(\log\) probability as a function of the truncation level (left). There are five clusters in the simulated 20 -dimensional DP mixture of Gaussians data set which was used. Held-out probability as a function of iteration of variational inference for the same simulated data set (right). The relative change in the log probability bound of the observations is labeled at selected iterations.}
\end{figure}

\begin{figure}
\includegraphics[max width=\textwidth]{https://cdn.mathpix.com/cropped/b4ca9d49-b8eb-48a5-8d65-5ba75d4210f0-16.jpg?height=296&width=1272&top_left_y=491&top_left_x=421}
\captionsetup{labelformat=empty}
\caption{Figure 5: Autocorrelation plots on the size of the largest component for the truncated DP Gibbs sampler (left) and collapsed Gibbs sampler (right) in an example dataset of 50-dimensional Gaussian data.}
\end{figure}

The TDP approximation was truncated at \(K=20\) components. For the variational algorithm, the truncation level was also \(T=20\) components. Note that in the latter case, the truncation level is simply another variational parameter. While we held \(T\) fixed in our simulations, it is also possible to optimize \(T\) with respect to the KL divergence. Indeed, Figure 4 (left) shows how the optimal KL divergence changes as a function of the truncation level for one of the simulated data sets.

We ran all algorithms to convergence and measured the computation time. \({ }^{3}\) For the collapsed Gibbs sampler, we assessed convergence to the stationary distribution with the diagnostic given by Raftery and Lewis (1992), and collected 25 additional samples to estimate the predictive distribution (the same diagnostic provides an appropriate lag at which to collect uncorrelated samples). We assessed convergence of the blocked Gibbs sampler using the same statistic as for the collapsed Gibbs sampler and used the same number of samples to form the approximate predictive distribution. \({ }^{4}\)

Finally, for variational inference, we measured convergence using the relative change in the log marginal probability bound (Equation 16), stopping the algorithm when it was less than \(1 e^{-10}\).

There is a certain inevitable arbitrariness in these choices; in general it is difficult to envisage measures of computation time that allow stochastic MCMC algorithms and deterministic variational algorithms to be compared in a standardized way. Nonetheless, we have made what we consider to be reasonable, pragmatic choices. In particular, our choice of stopping time for the variational algorithm is quite conservative, as illustrated in Figure 4 (right).

Figure 3 illustrates the average convergence time across ten datasets per dimension. With the caveats in mind regarding convergence time measurement, it appears that the variational algorithm is quite competitive with the MCMC algorithms. The variational

\footnotetext{
\({ }^{3}\) All timing computations were made on a Pentium III 1GHZ desktop machine.
\({ }^{4}\) Typically, hundreds or thousands of samples are used in MCMC algorithms to form the approximate posterior. However, we found that such approximations did not offer any additional predictive performance in the simulated data. To be fair to MCMC in the timing comparisons, we used a small number of samples to estimate the predictive distributions.
}

\begin{figure}
\includegraphics[max width=\textwidth]{https://cdn.mathpix.com/cropped/b4ca9d49-b8eb-48a5-8d65-5ba75d4210f0-17.jpg?height=553&width=1329&top_left_y=474&top_left_x=399}
\captionsetup{labelformat=empty}
\caption{Figure 6: Four sample clusters from a DP mixture analysis of 5000 images from the Associated Press. The left-most column is the posterior mean of each cluster followed by the top ten images associated with it. These clusters capture patterns in the data, such as basketball shots, outdoor scenes on gray days, faces, and pictures with blue backgrounds.}
\end{figure}
algorithm was faster and exhibited significantly less variance in its convergence time. Moreover, there is little evidence of an increase in convergence time across dimensionality for the variational algorithm over the range tested.

Note that the collapsed Gibbs sampler converged faster than the TDP Gibbs sampler. Though an iteration of collapsed Gibbs is slower than an iteration of TDP Gibbs, the TDP Gibbs sampler required a longer burn-in and greater lag to obtain uncorrelated samples. This is illustrated in the autocorrelation plots of Figure 5. Comparing the two MCMC algorithms, we found no advantage to the truncated approximation.

Table 1 illustrates the average log likelihood assigned to the held-out data by the approximate predictive distributions. First, notice that the collapsed DP Gibbs sampler assigned the same likelihood as the posterior from the TDP Gibbs sampler-an indication of the quality of a TDP for approximating a DP. More importantly, however, the predictive distribution based on the variational posterior assigned a similar score as those based on samples from the true posterior. Though it is based on an approximation to the posterior, the resulting predictive distributions are very accurate for this class of DP mixtures.

\section*{6 Image analysis}

Finite Gaussian mixture models are widely used in computer vision to model natural images for the purposes of automatic clustering, retrieval, and classification (Barnard et al. 2003; Jeon et al. 2003). These applications are often large-scale data analysis problems, involving thousands of data points (images) in hundreds of dimensions (pixels). The ap-

\begin{figure}
\includegraphics[max width=\textwidth]{https://cdn.mathpix.com/cropped/b4ca9d49-b8eb-48a5-8d65-5ba75d4210f0-18.jpg?height=426&width=1331&top_left_y=470&top_left_x=397}
\captionsetup{labelformat=empty}
\caption{Figure 7: The expected number of images allocated to each component in the variational posterior (left). The posterior uses 79 components to describe the data. The prior for the scaling parameter \(\alpha\) and the approximate posterior given by its variational distribution (right).}
\end{figure}
propriate number of mixture components to use in these problems is generally unknown, and DP mixtures provide an attractive alternative to current methods. However, a deployment of DP mixtures in such problems crucially requires inferential methods that are computationally efficient. To demonstrate the applicability of our variational approach to DP mixtures in the setting of large datasets, we analyzed a collection of 5000 images from the Associated Press under the assumptions of a DP mixture of Gaussians model.

Each image was reduced to a 192-dimensional real-valued vector given by an \(8 \times 8\) grid of average red, green, and blue values. We fit a DP mixture model in which the mixture components are Gaussian with mean \(\mu\) and covariance matrix \(\sigma^{2} I\). The base distribution \(G_{0}\) was a product measure- \(\operatorname{Gamma}(4,2)\) for \(1 / \sigma^{2}\) and \(\mathcal{N}\left(0,5 \sigma^{2}\right)\) for \(\mu\). Furthermore, we placed a \(\operatorname{Gamma}(1,1)\) prior on the DP scaling parameter \(\alpha\), as described in Appendix 7. We used a truncation level of 150 for the variational distribution.

The variational algorithm required approximately four hours to converge. The resulting approximate posterior used 79 mixture components to describe the collection. For a rough comparison to Gibbs sampling, an iteration of collapsed Gibbs takes 15 minutes with this data set. In the same four hours, one could perform only 16 iterations. This is not enough for a chain to converge to its stationary distribution, let alone provide a sufficient number of uncorrelated samples to construct an empirical estimate of the posterior.

Figure 7 (left) illustrates the expected number of images allocated to each component under the variational approximation to the posterior. Figure 6 illustrates the ten pictures with highest approximate posterior probability associated with each of four of the components. These clusters appear to capture basketball shots, outdoor scenes on gray days, faces, and blue backgrounds.

Figure 7 (right) illustrates the prior for the scaling parameter \(\alpha\) as well as the approximate posterior given by the fitted variational distribution. We see that the
approximate posterior is peaked and rather different from the prior, indicating that the data have provided information regarding \(\alpha\).

\section*{7 Conclusions}

We have developed a mean-field variational inference algorithm for the Dirichlet process mixture model and demonstrated its applicability to the kinds of multivariate data for which Gibbs sampling algorithms can exhibit slow convergence. Variational inference was faster than Gibbs sampling in our simulations, and its convergence time was independent of dimensionality for the range which we tested.

Both variational and MCMC methods have strengths and weaknesses, and it is unlikely that one methodology will dominate the other in general. While MCMC sampling provides theoretical guarantees of accuracy, variational inference provides a fast, deterministic approximation to otherwise unattainable posteriors. Moreover, both MCMC and variational methods are computational paradigms, providing a wide variety of specific algorithmic approaches which trade off speed, accuracy and ease of implementation in different ways. We have investigated the deployment of the simplest form of variational method for DP mixtures - a mean-field variational algorithm - but it worth noting that other variational approaches, such as those described in Wainwright and Jordan (2003), are also worthy of consideration in the nonparametric context.

\section*{Appendix-A Variational inference in exponential families}

In this appendix, we derive the coordinate ascent algorithm for variational inference described in Section 3.2. Recall that we are considering a latent variable model with hyperparameters \(\theta\), observed variables \(\mathbf{x}=\left\{x_{1}, \ldots, x_{N}\right\}\), and latent variables \(\mathbf{W}= \left\{W_{1}, \ldots, W_{M}\right\}\). The posterior can be written as
\[
p(\mathbf{w} \mid \mathbf{x}, \theta)=\exp \{\log p(\mathbf{w}, \mathbf{x} \mid \theta)-\log p(\mathbf{x} \mid \theta)\}
\]

The variational bound on the \(\log\) marginal probability is
\[
\log p(\mathbf{x} \mid \theta) \geq \mathrm{E}_{q}[\log p(\mathbf{x}, \mathbf{W} \mid \theta)]-\mathrm{E}_{q}[\log q(\mathbf{W})]
\]

This bound holds for any distribution \(q(\mathbf{w})\).
For the optimization of this bound to be computationally tractable, we restrict ourselves to fully-factorized variational distributions of the form \(q_{\boldsymbol{\nu}}(\mathbf{w})=\prod_{i=1}^{M} q_{\nu_{i}}\left(w_{i}\right)\), where \(\boldsymbol{\nu}=\left\{\nu_{1}, \nu_{2}, \ldots, \nu_{M}\right\}\) are variational parameters and each distribution is in the exponential family (Ghahramani and Beal 2001). We derive a coordinate ascent algorithm in which we iteratively maximize the bound with respect to each \(\nu_{i}\), holding the other variational parameters fixed.

Let us rewrite the bound in Equation (30) using the chain rule:
\[
\log p(\mathbf{x} \mid \theta) \geq \log p(\mathbf{x} \mid \theta)+\sum_{m=1}^{M} \mathrm{E}_{q}\left[\log p\left(W_{m} \mid \mathbf{x}, W_{1}, \ldots, W_{m-1}, \theta\right)\right]-\sum_{m=1}^{M} \mathrm{E}_{q}\left[\log q_{\nu_{m}}\left(W_{m}\right)\right]
\]

To optimize with respect to \(\nu_{i}\), reorder \(\mathbf{w}\) such that \(w_{i}\) is last in the list. The portion of Equation (31) depending on \(\nu_{i}\) is
\[
\ell_{i}=\mathrm{E}_{q}\left[\log p\left(W_{i} \mid \mathbf{W}_{-i}, \mathbf{x}, \theta\right)\right]-\mathrm{E}_{q}\left[\log q_{\nu_{i}}\left(W_{i}\right)\right]
\]

The variational distribution \(q_{\nu_{i}}\left(w_{i}\right)\) is in the exponential family,
\[
q_{\nu_{i}}\left(w_{i}\right)=h\left(w_{i}\right) \exp \left\{\nu_{i}^{T} w_{i}-a\left(\nu_{i}\right)\right\}
\]
and Equation (32) simplifies as follows:
\[
\begin{aligned}
\ell_{i} & =\mathrm{E}_{q}\left[\log p\left(W_{i} \mid \mathbf{W}_{-i}, \mathbf{x}, \theta\right)-\log h\left(W_{i}\right)-\nu_{i}^{T} W_{i}+a\left(\nu_{i}\right)\right] \\
& =\mathrm{E}_{q}\left[\log p\left(W_{i} \mid \mathbf{W}_{-i}, \mathbf{x}, \theta\right)\right]-\mathrm{E}_{q}\left[\log h\left(W_{i}\right)\right]-\nu_{i}^{T} a^{\prime}\left(\nu_{i}\right)+a\left(\nu_{i}\right),
\end{aligned}
\]
because \(\mathrm{E}_{q}\left[W_{i}\right]=a^{\prime}\left(\nu_{i}\right)\).
The derivative with respect to \(\nu_{i}\) is
\[
\frac{\partial}{\partial \nu_{i}} \ell_{i}=\frac{\partial}{\partial \nu_{i}}\left(\mathrm{E}_{q}\left[\log p\left(W_{i} \mid \mathbf{W}_{-i}, \mathbf{x}, \theta\right)\right]-\mathrm{E}_{q}\left[\log h\left(W_{i}\right)\right]\right)-\nu_{i}^{T} a^{\prime \prime}\left(\nu_{i}\right)
\]

The optimal \(\nu_{i}\) satisfies
\[
\nu_{i}=\left[a^{\prime \prime}\left(\nu_{i}\right)\right]^{-1}\left(\frac{\partial}{\partial \nu_{i}} \mathrm{E}_{q}\left[\log p\left(W_{i} \mid \mathbf{W}_{-i}, \mathbf{x}, \theta\right)\right]-\frac{\partial}{\partial \nu_{i}} \mathrm{E}_{q}\left[\log h\left(W_{i}\right)\right]\right)
\]

The result in Equation (34) is general. In many applications of mean field methods, including those in the current paper, a further simplification is achieved. In particular, if the conditional distribution \(p\left(w_{i} \mid \mathbf{w}_{-i}, \mathbf{x}, \theta\right)\) is an exponential family distribution then
\[
p\left(w_{i} \mid \mathbf{w}_{-i}, \mathbf{x}, \theta\right)=h\left(w_{i}\right) \exp \left\{g_{i}\left(\mathbf{w}_{-i}, \mathbf{x}, \theta\right)^{T} w_{i}-a\left(g_{i}\left(\mathbf{w}_{-i}, \mathbf{x}, \theta\right)\right)\right\}
\]
where \(g_{i}\left(\mathbf{w}_{-i}, \mathbf{x}, \theta\right)\) denotes the natural parameter for \(w_{i}\) when conditioning on the remaining latent variables and the observations. This yields simplified expressions for the expected log probability of \(W_{i}\) and its first derivative:
\[
\begin{aligned}
\mathrm{E}_{q}\left[\log p\left(W_{i} \mid \mathbf{W}_{-i}, \mathbf{x}, \theta\right)\right] & =\mathrm{E}_{q}\left[\log h\left(W_{i}\right)\right]+\mathrm{E}_{q}\left[g_{i}\left(\mathbf{W}_{-i}, \mathbf{x}, \theta\right)\right]^{T} a^{\prime}\left(\nu_{i}\right)-\mathrm{E}_{q}\left[a\left(g_{i}\left(\mathbf{W}_{-i}, \mathbf{x}, \theta\right)\right)\right] \\
\frac{\partial}{\partial \nu_{i}} \mathrm{E}_{q}\left[\log p\left(W_{i} \mid \mathbf{W}_{-i}, \mathbf{x}, \theta\right)\right] & =\frac{\partial}{\partial \nu_{i}} \mathrm{E}_{q}\left[\log h\left(W_{i}\right)\right]+\mathrm{E}_{q}\left[g_{i}\left(\mathbf{W}_{-i}, \mathbf{x}, \theta\right)\right]^{T} a^{\prime \prime}\left(\nu_{i}\right)
\end{aligned}
\]

Using the first derivative in Equation (34), the maximum is attained at
\[
\nu_{i}=\mathrm{E}_{q}\left[g_{i}\left(\mathbf{W}_{-i}, \mathbf{x}, \theta\right)\right]
\]

We define a coordinate ascent algorithm based on Equation (35) by iteratively updating \(\nu_{i}\) for \(i \in\{1, \ldots, M\}\). Such an algorithm finds a local maximum of Equation (30) by Proposition 2.7.1 of Bertsekas (1999), under the condition that the right-hand side of Equation (32) is strictly convex.

Relaxing the two assumptions complicates the algorithm, but the basic idea remains the same. If \(p\left(w_{i} \mid \mathbf{w}_{-i}, \mathbf{x}, \theta\right)\) is not in the exponential family, then there may not be an analytic expression for the update in Equation (34). If \(q(\mathbf{w})\) is not a fully factorized distribution, then the second term of the bound in Equation (32) becomes \(\mathrm{E}_{q}\left[\log q\left(w_{i} \mid \mathbf{w}_{-i}\right)\right]\) and the subsequent simplifications may not be applicable.

Further perspectives on algorithms of this kind can be found in Xing et al. (2003), Ghahramani and Beal (2001), and Wiegerinck (2000). For a more general treatment of variational methods for statistical inference, see Wainwright and Jordan (2003).

\section*{Appendix-B Placing a prior on the scaling parameter}

The scaling parameter \(\alpha\) can have a significant effect on the growth of the number of components grows with the data, and it is generally important to consider extended models which integrate over \(\alpha\). For the urn-based samplers, Escobar and West (1995) place a \(\operatorname{Gamma}\left(s_{1}, s_{2}\right)\) prior on \(\alpha\) and implement the corresponding Gibbs updates with auxiliary variable methods.

In the stick-breaking representation, the gamma distribution is convenient because it is conjugate to the stick lengths. We write the gamma distribution in its canonical form:
\[
p\left(\alpha \mid s_{1}, s_{2}\right)=(1 / \alpha) \exp \left\{-s_{2} \alpha+s_{1} \log \alpha-a\left(s_{1}, s_{2}\right)\right\}
\]
where \(s_{1}\) is the shape parameter and \(s_{2}\) is the inverse scale parameter. This distribution is conjugate to \(\operatorname{Beta}(1, \alpha)\). The log normalizer is
\[
a\left(s_{1}, s_{2}\right)=\log \Gamma\left(s_{1}\right)-s_{1} \log s_{2}
\]
and the posterior parameters conditional on data \(\left\{v_{1}, \ldots, v_{K}\right\}\) are
\[
\begin{aligned}
\hat{s}_{2} & =s_{2}-\sum_{i=1}^{K} \log \left(1-v_{i}\right) \\
\hat{s}_{1} & =s_{1}+K
\end{aligned}
\]

We extend the variational inference algorithm to include posterior updates for the scaling parameter \(\alpha\). The variational distribution is \(\operatorname{Gamma}\left(w_{1}, w_{2}\right)\). The variational parameters are updated as follows:
\[
\begin{aligned}
& w_{1}=s_{1}+T-1 \\
& \left.w_{2}=s_{2}-\sum_{i=1}^{T-1} \mathrm{E}_{q}\left[\log \left(1-V_{i}\right)\right]\right),
\end{aligned}
\]
and we replace \(\alpha\) with its expectation \(\mathrm{E}_{q}[\alpha]=w_{1} / w_{2}\) in the updates for \(\gamma_{t, 2}\) in Equation (19).

\section*{Bibliography}

Antoniak, C. (1974). "Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems." The Annals of Statistics, 2(6):1152-1174. 122, 124

Attias, H. (2000). "A variational Bayesian framework for graphical models." In Solla, S., Leen, T., and Muller, K. (eds.), Advances in Neural Information Processing Systems 12, 209-215. Cambridge, MA: MIT Press. 121

Barnard, K., Duygulu, P., de Freitas, N., Forsyth, D., Blei, D., and Jordan, M. (2003). "Matching words and pictures." Journal of Machine Learning Research, 3:1107-1135. 137

Bertsekas, D. (1999). Nonlinear Programming. Nashua, NH: Athena Scientific. 141
Blackwell, D. and MacQueen, J. (1973). "Ferguson distributions via P贸lya urn schemes." The Annals of Statistics, 1(2):353-355. 122, 123

Blei, D., Ng, A., and Jordan, M. (2003). "Latent Dirichlet allocation." Journal of Machine Learning Research, 3:993-1022. 121

Connor, R. and Mosimann, J. (1969). "Concepts of independence for proportions with a generalization of the Dirichlet distribution." Journal of the American Statistical Association, 64(325):194-206. 132

Escobar, M. and West, M. (1995). "Bayesian density estimation and inference using mixtures." Journal of the American Statistical Association, 90:577-588. 122, 125, 141

Ferguson, T. (1973). "A Bayesian analysis of some nonparametric problems." The Annals of Statistics, 1:209-230. 122, 123, 124

Ghahramani, Z. and Beal, M. (2001). "Propagation algorithms for variational Bayesian learning." In Leen, T., Dietterich, T., and Tresp, V. (eds.), Advances in Neural Information Processing Systems 13, 507-513. Cambridge, MA: MIT Press. 121, 122, 127, 139, 141

Ishwaran, J. and James, L. (2001). "Gibbs sampling methods for stick-breaking priors." Journal of the American Statistical Association, 96:161-174. 125, 128, 131

Jeon, J., Lavrenko, V., and Manmatha, R. (2003). "Automatic image annotation and retrieval using cross-media relevance models." In Proceedings of the 26th Annual International ACM SIGIR conference on Research and Development in Information Retrieval, 119-126. ACM Press. 137

Jordan, M., Ghahramani, Z., Jaakkola, T., and Saul, L. (1999). "Introduction to variational methods for graphical models." Machine Learning, 37:183-233. 121

MacEachern, S. (1994). "Estimating normal means with a conjugate style Dirichlet process prior." Communications in Statistics B, 23:727-741. 122, 125, 130
- (1998). "Computational methods for mixture of Dirichlet process models." In Dey, D., Muller, P., and Sinha, D. (eds.), Practical Nonparametric and Semiparametric Bayesian Statistics, 23-44. Springer. 125

Neal, R. (2000). "Markov chain sampling methods for Dirichlet process mixture models." Journal of Computational and Graphical Statistics, 9(2):249-265. 122, 125, 131

Opper, M. and Saad, D. (2001). Advanced Mean Field Methods: Theory and Practice. Cambridge, MA: MIT Press. 121

Raftery, A. and Lewis, S. (1992). "One long run with diagnostics: Implementation strategies for Markov chain Monte Carlo." Statistical Science, 7:493-497. 136

Robert, C. and Casella, G. (2004). Monte Carlo Statistical Methods. New York, NY: Springer-Verlag. 133

Sethuraman, J. (1994). "A constructive definition of Dirichlet priors." Statistica Sinica, 4:639-650. 122, 124

Wainwright, M. and Jordan, M. (2003). "Graphical models, exponential families, and variational inference." Technical Report 649, U.C. Berkeley, Dept. of Statistics. 121, 122, 125, 126, 139, 141

Wiegerinck, W. (2000). "Variational approximations between mean field theory and the junction tree algorithm." In Boutilier, C. and Goldszmidt, M. (eds.), Proceedings of the 16th Annual Conference on Uncertainty in Artificial Intelligence (UAI-00), 626-633. San Francisco, CA: Morgan Kaufmann Publishers. 121, 141

Xing, E., Jordan, M., and Russell, S. (2003). "A generalized mean field algorithm for variational inference in exponential families." In Meek, C. and Kj忙rulff, U. (eds.), Proceedings of the 19th Annual Conference on Uncertainty in Artificial Intelligence (UAI-03), 583-591. San Francisco, CA: Morgan Kaufmann Publishers. 141

\section*{Acknowledgments}

We thank Jaety Edwards for providing the AP image data. We want to acknowledge support from Intel Corporation, Microsoft Research, and a grant from DARPA in support of the CALO project.